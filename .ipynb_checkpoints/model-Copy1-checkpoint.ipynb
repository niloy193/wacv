{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.hrnet import HighResolutionNet, hrnet18\n",
    "from model.config import HRNET_32\n",
    "from model.unet import UNet\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "with open('config/config.yaml', 'r') as file:\n",
    "    cfg = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import timm\n",
    "from model.aspp import build_aspp\n",
    "from model.srm import setup_srm_layer\n",
    "class ConSegNet(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(ConSegNet, self).__init__()\n",
    "        self.encoder = timm.create_model(cfg['model_params']['encoder'], pretrained= False, features_only=True, out_indices=[-1])\n",
    "        self.conv_srm = setup_srm_layer()\n",
    "        self.encoder_srm = timm.create_model(cfg['model_params']['encoder'], pretrained= False, features_only=True, out_indices=[-1])\n",
    "        if cfg['global_params']['with_srm'] == True:\n",
    "            self.aspp = build_aspp(inplanes = cfg['model_params']['aspp_inplane']*2, outplanes = cfg['model_params']['aspp_outplane'] )\n",
    "        else:\n",
    "            self.aspp = build_aspp(inplanes = cfg['model_params']['aspp_inplane'], outplanes = cfg['model_params']['aspp_outplane'] )\n",
    "        \n",
    "            \n",
    "        self.decoder = nn.Sequential(nn.Conv2d(512, 512, kernel_size=3,\n",
    "                                              padding=1, stride=1, bias=False),\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.Conv2d(512, cfg['model_params']['num_class'], kernel_size=1,\n",
    "                                              stride=1, bias=True))\n",
    "        self.projection = nn.Sequential(\n",
    "                nn.Conv2d(512, 512, kernel_size=1),\n",
    "                nn.BatchNorm2d(512),\n",
    "                nn.Conv2d(512, 256, kernel_size=1)\n",
    "            )\n",
    "    def forward(self, inp):\n",
    "        x = self.encoder(inp)[0]\n",
    "\n",
    "        if cfg['global_params']['with_srm'] == True:\n",
    "            x_srm = self.conv_srm(inp)\n",
    "            x_srm = self.encoder_srm(x_srm)[0]\n",
    "            x = torch.concat([x, x_srm], dim=1)\n",
    "\n",
    "        x = self.aspp(x)\n",
    "        x = F.interpolate(x, scale_factor = 4, mode = 'bilinear', align_corners= True)\n",
    "        out = self.decoder(x)\n",
    "        proj = self.projection(x)\n",
    "        return out, proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((2,3,256,256)).cuda()\n",
    "model = ConSegNet(cfg).cuda()\n",
    "out, proj = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = HighResolutionNet(HRNET_32).cuda()\n",
    "\n",
    "inp = torch.randn((1,64,224,224)).cuda()\n",
    "B,C,H,W = inp.shape\n",
    "# print(inp)\n",
    "inp = inp.contiguous().view((B,C,-1))\n",
    "# print(inp2)\n",
    "inp = torch.moveaxis(inp, 1, -1)\n",
    "feat = torch.matmul(inp, torch.transpose(inp, 1,2)).float()\n",
    "inp = 0\n",
    "print(feat.shape)\n",
    "\n",
    "target = torch.randint(0,2,(1,224,224)).cuda()\n",
    "labels = target.contiguous().view(B,-1,1)\n",
    "\n",
    "\n",
    "pos_labels = torch.eq(labels, labels.transpose(1,2)).float() #poslabels in case of forged pixels\n",
    "neg_labels = torch.add(torch.negative(pos_labels),1)\n",
    "\n",
    "temperature = 0.6\n",
    "feat =  torch.div(feat,temperature)\n",
    "feat = torch.exp(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splice regions:\n",
    "neg_sum = torch.sum(torch.mul(feat, neg_labels), dim=-1).contiguous().view(B,-1,1)\n",
    "denominator = torch.add(feat, neg_sum)\n",
    "division = torch.div(feat, denominator+ 1e-10)\n",
    "loss_matrix = -torch.log(division+1e-10)\n",
    "loss_matrix = torch.mul(loss_matrix , pos_labels)\n",
    "loss_matrix = torch.mul(loss_matrix , torch.add(torch.negative(torch.eye(loss_matrix.shape[-1])),1))\n",
    "\n",
    "loss = torch.sum(loss_matrix, dim=-1)\n",
    "# # loss1 = torch.div(loss1, mem_cardinality)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spliced_loss = torch.mul(loss, labels.view(B,-1))\n",
    "pristine_loss = torch.mul(loss, torch.add(torch.negative(labels),1).view(B,-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "print(torch.add(torch.negative(labels),1).view(B,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.add(torch.negative(torch.eye(loss_matrix.shape[-1])),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = feat = torch.randn((3,2,5))\n",
    "labels = torch.randint(0,2,(3,))\n",
    "\n",
    "temperature = 0.2\n",
    "contrast_mode = 'all'\n",
    "base_temperature = 0.2\n",
    "\n",
    "batch_size = features.shape[0]\n",
    "labels = labels.contiguous().view(-1, 1)\n",
    "\n",
    "mask = torch.eq(labels, labels.T).float()\n",
    "   \n",
    "\n",
    "\n",
    "contrast_count = features.shape[1] #basically number of views, \n",
    "\n",
    "\n",
    "contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "if contrast_mode == 'one':\n",
    "    anchor_feature = features[:, 0]\n",
    "    anchor_count = 1\n",
    "elif contrast_mode == 'all':\n",
    "    anchor_feature = contrast_feature #long length vector of batch*no_o_views\n",
    "    anchor_count = contrast_count #number of views\n",
    "    \n",
    "\n",
    "    # compute logits\n",
    "anchor_dot_contrast = torch.div(\n",
    "    torch.matmul(anchor_feature, contrast_feature.T),\n",
    "    temperature)\n",
    "\n",
    "# for numerical stability\n",
    "logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "# tile mask\n",
    "mask = mask.repeat(anchor_count, contrast_count)\n",
    "# mask-out self-contrast cases\n",
    "logits_mask = torch.scatter(\n",
    "    torch.ones_like(mask),\n",
    "    1,\n",
    "    torch.arange(batch_size * anchor_count).view(-1, 1),\n",
    "    0\n",
    ")\n",
    "\n",
    "mask = mask * logits_mask\n",
    "\n",
    "# compute log_prob\n",
    "exp_logits = torch.exp(logits) * logits_mask\n",
    "log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "print(log_prob.shape)\n",
    "\n",
    "# compute mean of log-likelihood over positive\n",
    "mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "print(mean_log_prob_pos.shape)\n",
    "loss = - (temperature / base_temperature) * mean_log_prob_pos\n",
    "loss = loss.view(anchor_count, batch_size).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class memory_bank():\n",
    "    def __init__(self, length, n_class, featlength, device):\n",
    "        self.memory = torch.randn(length, n_class, featlength)\n",
    "        if device:\n",
    "            self.memory = self.memory.to(device)\n",
    "\n",
    "    def enqueue(self,feat):\n",
    "        batch_size = feat.shape[0]\n",
    "        self.memory = torch.cat((feat, self.memory), dim=0)[:-batch_size]\n",
    "        \n",
    "mem = memory_bank(200,3,64)\n",
    "feat = torch.randn((8,3,64))\n",
    "\n",
    "mem.enqueue(feat)\n",
    "print(mem.memory.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = torch.randn((1024,3,64)).cuda()\n",
    "mem = torch.randn((2048,3,64)).cuda()\n",
    "\n",
    "temperature = 0.6\n",
    "mem_cardinality = mem.shape[0]\n",
    "\n",
    "feat_labels = torch.arange(0,feat.shape[1],1).contiguous().view(-1,1)\n",
    "mem_labels = torch.arange(0,mem.shape[1],1).contiguous().view(-1,1)\n",
    "\n",
    "feat_labels = feat_labels.repeat((1,feat.shape[0])).contiguous().view(-1,1)\n",
    "mem_labels = mem_labels.repeat((1,mem.shape[0])).contiguous().view(-1,1)\n",
    "\n",
    "feat = torch.cat(torch.unbind(feat, dim=1), dim=0)\n",
    "mem = torch.cat(torch.unbind(mem, dim=1), dim=0)\n",
    "\n",
    "# feat_mask = torch.eq(feat_labels, feat_labels.T).float().cuda()\n",
    "mem_mask = torch.eq(feat_labels, mem_labels.T).float().cuda()\n",
    "\n",
    "\n",
    "# feat_mask_neg = torch.add(torch.negative(feat_mask),1).cuda()\n",
    "mem_mask_neg = torch.add(torch.negative(mem_mask),1).cuda()\n",
    "\n",
    "feat_logits =  torch.div(torch.matmul(feat, feat.T),temperature)\n",
    "mem_logits = torch.div(torch.matmul(feat, mem.T),temperature)\n",
    "\n",
    "# for stability\n",
    "feat_logits_max, _ = torch.max(feat_logits, dim=1, keepdim=True)\n",
    "feat_logits = feat_logits - feat_logits_max.detach()\n",
    "mem_logits_max, _ = torch.max(mem_logits, dim=1, keepdim=True)\n",
    "mem_logits = mem_logits - mem_logits_max.detach()\n",
    "\n",
    "feat_logits = torch.exp(feat_logits)\n",
    "mem_logits = torch.exp(mem_logits)\n",
    "\n",
    "neg_sum = torch.sum(torch.mul(mem_logits, mem_mask_neg), dim=-1).contiguous().view(-1,1)\n",
    "denominator = torch.add(mem_logits, neg_sum)\n",
    "division = torch.div(mem_logits, denominator+ 1e-10)\n",
    "loss_matrix = -torch.log(division+1e-10)\n",
    "loss_matrix = torch.mul(loss_matrix , mem_mask)\n",
    "loss = torch.sum(loss_matrix, dim=-1)\n",
    "loss = torch.div(loss, mem_cardinality)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e470db166faf928c816a3c4594623f758f5f8b390569cb68da693a2d6ba357bd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
